{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sp24U2f_Xlqp"
   },
   "source": [
    "# Adaptive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fktPg5voEllg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "### Build Index\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "\n",
    "###### router import\n",
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b00LfpzY4HA"
   },
   "source": [
    "### Build DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3p0y2OGlu8OV",
    "outputId": "1b84d685-3d90-418a-ff13-189696160cf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectoselected <langchain_community.vectorstores.chroma.Chroma object at 0x12f96ffe0>\n"
     ]
    }
   ],
   "source": [
    "embedding_function = OpenAIEmbeddings()\n",
    "\n",
    "urls = [\n",
    "        \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "        \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "    ]\n",
    "\n",
    "# Load\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    " # Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=512, chunk_overlap=0)\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=embedding_function\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "print(\"vectoselected\",vectorstore)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EEWVkkwZfWd"
   },
   "source": [
    "### Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S72zdGy2vPlb",
    "outputId": "34af3b4a-706e-486e-d5e0-dd80622cb51c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='web_search'\n",
      "datasource='vectorstore'\n"
     ]
    }
   ],
   "source": [
    "### Build Index\n",
    "### Router\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"你是一個專家，負責將用戶問題路由到向量資料庫或網絡搜索。\n",
    "向量資料庫包含與代理、提示工程和對抗性攻擊相關的文檔。\n",
    "對於這些主題的問題使用向量資料庫。否則，使用網絡搜索。\"\"\"\n",
    "route_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"{question}\")])\n",
    "question_router = route_prompt | structured_llm_router\n",
    "\n",
    "\n",
    "question1 =\"李多慧什麼時候加入統一獅？\"\n",
    "print(question_router.invoke({\"question\": question1}))\n",
    "print(question_router.invoke({\"question\": \"請問 Agent 是什麼？\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbCfWEGZ9wSh"
   },
   "source": [
    "### Retrieval Grader\n",
    "這個組件用於評估檢索文檔與問題的相關性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67NphMjHZN5M",
    "outputId": "b77b783b-010d-4c1d-e151-b1e579fafad4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yw/v6fz89fx5pd827bs1070ywr00000gn/T/ipykernel_73111/999480785.py:21: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"你是一個評分員，評估檢索文檔與用戶問題的相關性。\n",
    "如果文檔包含與用戶問題相關的關鍵詞或語義含義，將其評為相關。\n",
    "這不需要是嚴格的測試。目的是過濾掉錯誤的檢索結果。\n",
    "給出二元評分 'yes' 或 'no' 來表示文檔是否與問題相關。\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"檢索文檔：\\n\\n {document} \\n\\n 用戶問題：{question}\")])\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "question = \"agent memory\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nq11QGniGIkY"
   },
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, page_content):\n",
    "        self.page_content = page_content\n",
    "    def __repr__(self):\n",
    "        return f\"Document(page_content='{self.page_content}')\"\n",
    "\n",
    "cleaned_docs_format = [Document(doc.page_content) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm5_TFuR95D0"
   },
   "source": [
    "### Generate Node\n",
    "用途：生成節點負責基於檢索的文檔生成答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TGKO4lBUZN74",
    "outputId": "28c9753f-3c2c-4995-fc63-86ce74affa65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent memory includes a long-term memory module that records agents' experiences in natural language. It combines memory, planning, and reflection mechanisms to enable agents to behave based on past experiences and interact with other agents. The retrieval model surfaces context based on relevance, recency, and importance to inform the agent's behavior.\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "(\"human\", \"\"\"你是一個問答任務的助手。使用以下檢索的上下文來回答問題。如果你不知道答案，就說不知道。最多使用三個句子，保持答案簡潔。\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\"\"\")])\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# # Run\n",
    "generation = rag_chain.invoke({\"context\": cleaned_docs_format, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5BODaie98lV"
   },
   "source": [
    "### Hallucination Grader\n",
    "這個組件用於評估生成的答案是否基於事實："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fZ39RkqtZN-v",
    "outputId": "d600b4e0-3af7-445e-eead-ffa4495d6e59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='yes')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "    binary_score: str = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"你是一個評分員，評估 LLM 生成的內容是否基於/支持於一組檢索的事實。\n",
    "給出二元評分 'yes' 或 'no'。'Yes' 表示答案基於/支持於這組事實。\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"事實集：\\n\\n {documents} \\n\\n LLM 生成：{generation}\")])\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "hallucination_grader.invoke({\"documents\": cleaned_docs_format, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgJ8wN0d-C1u"
   },
   "source": [
    "### Answer Grader\n",
    "用於評估生成的答案是否解決了問題："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkQTpm2fD53I",
    "outputId": "a5c23632-a708-4950-bf95-eda85140bef9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='no')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "    binary_score: str = Field(description=\"Answer addresses the question, 'yes' or 'no'\")\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"你是一個評分員，評估答案是否解決/回答了問題。\n",
    "給出二元評分 'yes' 或 'no'。\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"用戶問題：\\n\\n {question} \\n\\n LLM 生成：{generation}\")])\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader\n",
    "answer_grader.invoke({\"question\": question,\"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUnzdz8v-KZX"
   },
   "source": [
    "### Question Re-writer\n",
    "這個組件用於改寫用戶問題，使其更適合檢索："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "b1OUFY3sZOBZ",
    "outputId": "18774400-6db3-44ee-85fe-667aafd8d99c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'改寫後的問題：\\n\\n如何有效地管理代理人的記憶？'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"你是一個問題重寫器，將輸入問題轉換為更適合向量資料庫檢索的版本。\n",
    "分析輸入並嘗試理解其底層語義意圖/含義。\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"這是初始問題：\\n\\n {question} \\n 制定一個改進的問題。\")])\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fVyehT7sEKMP"
   },
   "outputs": [],
   "source": [
    "### Search\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11713gIH-Rzi"
   },
   "source": [
    "### Define State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1SJqOmndZOJk"
   },
   "outputs": [],
   "source": [
    "#################\n",
    "#langgraph code\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question : str\n",
    "    generation : str\n",
    "    documents : List[str]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "um1NgmFW-bWx"
   },
   "source": [
    "## Define Nodes\n",
    "\n",
    "包括檢索、生成、文檔評分、查詢轉換和網絡搜索。這些函數將在我們的圖中作為節點使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qp4tONlW-feu"
   },
   "outputs": [],
   "source": [
    "## graph flow\n",
    "def retrieve(state):\n",
    "    print(\"--- RETRIEVE DOCS ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "\n",
    "    # docs = retriever.get_relevant_documents(question)\n",
    "    # documents = [doc.page_content for doc in docs]\n",
    "\n",
    "    # docsNEW = [doc.page_content for doc in docs]  # Adjust if the actual attribute name differs\n",
    "    print(\"TAKING DICSNEW >>>>>>>>\")\n",
    "    return {\"documents\": cleaned_docs_format, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    print(\"--- GENERATE ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    print(\"--- GRADE DOCS ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"--- [GRADE]: RELEVANCE ---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"--- [GRADE]: NOT RELEVANCE ---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "def transform_query(state):\n",
    "    print(\"--- [RE-WRITE] ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "def web_search(state):\n",
    "    print(\"--- WEB SEARCH ---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "\n",
    "    return {\"documents\": web_results, \"question\": question}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaOpBz2r-fRy"
   },
   "source": [
    "### Define Conditional Edge\n",
    "用於決定下一步應該執行哪個節點。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4SZmdvtOZOMG"
   },
   "outputs": [],
   "source": [
    "### Edges ###\n",
    "\n",
    "def route_question(state):\n",
    "    print(\"--- ROUTING ---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    if source.datasource == 'web_search':\n",
    "        print(\"--- ROUTING -> WEB SEARCH ---\")\n",
    "        return \"web_search\"\n",
    "    elif source.datasource == 'vectorstore':\n",
    "        print(\"--- ROUTING -> RAG ---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    print(\"--- ASSESS GRADED DOCUMENTS ---\")\n",
    "    question = state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"--- DECISION: RE-WIRTE ---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"--- DECISION: GENERATE ---\")\n",
    "        return \"generate\"\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    print(\"--- CHECK HALLUCINATIONS ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"--- DECISION: GENERATION IS GROUNDED IN DOCUMENTS ---\")\n",
    "        print(\"--- GRADE GENERATION vs QUESTION ---\")\n",
    "        score = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"--- DECISION: GENERATION ADDRESSES QUESTION ---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"--- DECISION: GENERATION DOES NOT ADDRESS QUESTION ---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"--- DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY ---\")\n",
    "        return \"not supported\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gbia8AtM-lQg"
   },
   "source": [
    "### Build Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "PGWAX0mN-oN6"
   },
   "outputs": [],
   "source": [
    "## grapj build\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"web_search\", web_search) # web search\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents) # grade documents\n",
    "workflow.add_node(\"generate\", generate) # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query) # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaqYUlFV-pmM"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cqUImpZ6_-u",
    "outputId": "3771f7b4-e5d4-466f-c76d-f54818d6bdde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ROUTING ---\n",
      "--- ROUTING -> RAG ---\n",
      "--- RETRIEVE DOCS ---\n",
      "TAKING DICSNEW >>>>>>>>\n",
      "Node 'retrieve':\n",
      "\n",
      "--- GRADE DOCS ---\n",
      "--- [GRADE]: RELEVANCE ---\n",
      "--- [GRADE]: RELEVANCE ---\n",
      "--- [GRADE]: RELEVANCE ---\n",
      "--- [GRADE]: RELEVANCE ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "--- DECISION: GENERATE ---\n",
      "Node 'grade_documents':\n",
      "\n",
      "--- GENERATE ---\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "--- DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY ---\n",
      "Node 'generate':\n",
      "\n",
      "--- GENERATE ---\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "--- DECISION: GENERATION IS GROUNDED IN DOCUMENTS ---\n",
      "--- GRADE GENERATION vs QUESTION ---\n",
      "--- DECISION: GENERATION DOES NOT ADDRESS QUESTION ---\n",
      "Node 'generate':\n",
      "\n",
      "--- [RE-WRITE] ---\n",
      "Node 'transform_query':\n",
      "\n",
      "--- RETRIEVE DOCS ---\n",
      "TAKING DICSNEW >>>>>>>>\n",
      "Node 'retrieve':\n",
      "\n",
      "--- GRADE DOCS ---\n",
      "--- [GRADE]: RELEVANCE ---\n",
      "--- [GRADE]: RELEVANCE ---\n",
      "--- [GRADE]: RELEVANCE ---\n",
      "--- [GRADE]: RELEVANCE ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "--- DECISION: GENERATE ---\n",
      "Node 'grade_documents':\n",
      "\n",
      "--- GENERATE ---\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "--- DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY ---\n",
      "Node 'generate':\n",
      "\n",
      "--- GENERATE ---\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "--- DECISION: GENERATION IS GROUNDED IN DOCUMENTS ---\n",
      "--- GRADE GENERATION vs QUESTION ---\n",
      "--- DECISION: GENERATION ADDRESSES QUESTION ---\n",
      "Node 'generate':\n",
      "\n",
      "Agent 是指一個由LLM驅動的虛擬角色，可以在互動應用程序中創建人類行為的逼真模擬。這些代理人結合了記憶、規劃和反思機制，以根據過去的經驗行為，並與其他代理人互動。代理人還可以通過調用外部API來獲取缺失的信息。\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"什麼是 Agent ?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Node '{key}':\")\n",
    "    print('')\n",
    "# Final generation\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Eh2KhT-ZmpW",
    "outputId": "24d8d439-1132-419c-d609-b79448150a65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ROUTING ---\n",
      "--- ROUTING -> WEB SEARCH ---\n",
      "--- WEB SEARCH ---\n",
      "Node 'web_search':\n",
      "\n",
      "--- GENERATE ---\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "--- DECISION: GENERATION IS GROUNDED IN DOCUMENTS ---\n",
      "--- GRADE GENERATION vs QUESTION ---\n",
      "--- DECISION: GENERATION ADDRESSES QUESTION ---\n",
      "Node 'generate':\n",
      "\n",
      "美國建國總統是喬治·華盛頓。\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"誰是美國建國總統？\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node//\n",
    "        print(f\"Node '{key}':\")\n",
    "    print('')\n",
    "\n",
    "# Final generation\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3umALgi9YFC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "YL3sSzhNYrrL"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
